{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7694498,"sourceType":"datasetVersion","datasetId":4490776}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pdfminer.six\n!pip install gensim\n!pip install nltk\n!pip install spacy","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:54:26.463687Z","iopub.execute_input":"2024-02-29T23:54:26.467387Z","iopub.status.idle":"2024-02-29T23:55:28.707851Z","shell.execute_reply.started":"2024-02-29T23:54:26.467284Z","shell.execute_reply":"2024-02-29T23:55:28.705666Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pdfminer.six in /opt/conda/lib/python3.10/site-packages (20231228)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six) (41.0.7)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.24.4)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:53:20.378662Z","iopub.execute_input":"2024-02-29T23:53:20.379079Z","iopub.status.idle":"2024-02-29T23:53:21.427722Z","shell.execute_reply.started":"2024-02-29T23:53:20.379047Z","shell.execute_reply":"2024-02-29T23:53:21.426585Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport requests\nimport os\nimport re\nfrom pdfminer.high_level import extract_text\nimport glob\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim import corpora, models\nimport gensim\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2024-03-01T00:01:43.068841Z","iopub.execute_input":"2024-03-01T00:01:43.069270Z","iopub.status.idle":"2024-03-01T00:01:43.078523Z","shell.execute_reply.started":"2024-03-01T00:01:43.069239Z","shell.execute_reply":"2024-03-01T00:01:43.076660Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:55:34.264413Z","iopub.execute_input":"2024-02-29T23:55:34.265094Z","iopub.status.idle":"2024-02-29T23:55:35.764098Z","shell.execute_reply.started":"2024-02-29T23:55:34.265056Z","shell.execute_reply":"2024-02-29T23:55:35.762861Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Remove non-alphanumeric characters and extra whitespaces\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Lemmatize text using spaCy\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS]\n    \n    return lemmatized_tokens","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:58:34.876655Z","iopub.execute_input":"2024-02-29T23:58:34.877087Z","iopub.status.idle":"2024-02-29T23:58:34.886421Z","shell.execute_reply.started":"2024-02-29T23:58:34.877056Z","shell.execute_reply":"2024-02-29T23:58:34.884956Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def clean_and_preprocess_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read()\n    \n    # Remove non-alphanumeric characters and extra whitespaces\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Preprocess text\n    tokens = preprocess_text(text)\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:53:41.826309Z","iopub.execute_input":"2024-02-29T23:53:41.826799Z","iopub.status.idle":"2024-02-29T23:53:41.835643Z","shell.execute_reply.started":"2024-02-29T23:53:41.826759Z","shell.execute_reply":"2024-02-29T23:53:41.834193Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def clean_and_preprocess_directory(directory='/kaggle/working/txt_directory'):\n    documents = {}\n    for filename in os.listdir(directory):\n        if filename.endswith(\".txt\"):\n            file_path = os.path.join(directory, filename)\n            tokens = clean_and_preprocess_file(file_path)\n            documents[filename] = tokens\n    return documents","metadata":{"execution":{"iopub.status.busy":"2024-03-01T00:07:27.355479Z","iopub.execute_input":"2024-03-01T00:07:27.356001Z","iopub.status.idle":"2024-03-01T00:07:27.363316Z","shell.execute_reply.started":"2024-03-01T00:07:27.355967Z","shell.execute_reply":"2024-03-01T00:07:27.362057Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Increase the threshold with more documents\ndef perform_topic_modeling(directory='/kaggle/working/txt_directory', num_topics=5, max_freq_threshold=1000):\n    # Clean and preprocess documents\n    documents = clean_and_preprocess_directory(directory)\n    \n    # Count word frequencies across all documents\n    all_words = [word for doc in documents.values() for word in doc]\n    word_counts = Counter(all_words)\n    \n    # Identify words to be filtered based on their frequency\n    frequent_words = [word for word, freq in word_counts.items() if freq >= max_freq_threshold * len(all_words)]\n    \n    # Remove frequent words from documents\n    cleaned_documents = {file_name: [word for word in doc if word not in frequent_words] for file_name, doc in documents.items()}\n    \n    # Create dictionary and corpus\n    dictionary = corpora.Dictionary(cleaned_documents.values())\n    corpus = [dictionary.doc2bow(doc) for doc in cleaned_documents.values()]\n    \n    # Build LDA model\n    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n    \n    # Get topics for each document\n    document_topics = {}\n    for file_name, doc_bow in zip(cleaned_documents.keys(), corpus):\n        topics = lda_model.get_document_topics(doc_bow)\n        document_topics[file_name] = topics\n        \n    for idx, topic in lda_model.print_topics(-1):\n        print(f'Topic {idx}: {topic}\\n')\n    \n    return lda_model, document_topics","metadata":{"execution":{"iopub.status.busy":"2024-03-01T00:08:50.167471Z","iopub.execute_input":"2024-03-01T00:08:50.167943Z","iopub.status.idle":"2024-03-01T00:08:50.181432Z","shell.execute_reply.started":"2024-03-01T00:08:50.167911Z","shell.execute_reply":"2024-03-01T00:08:50.179918Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"glob.glob('/kaggle/working/pdf_directory/*.pdf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_pickle('/kaggle/input/semantic-scholar-retrieval/retrieved_data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_pdf_from_response(df_row, pdf_directory='/kaggle/working/pdf_directory'):\n    \"\"\"\n    This function takes the structure of the 'openAccessPdf' column of the semantic scholar\n    api response dataset downloads the available pdfs in the specified directory, indexed\n    with the paperid value.\n    \n    variables:\n    - df_row: And iterable with a 'paperId' key and an 'openAccessPdf' key\n    - pdf_directory: The folder in which the files will be stored\n    \"\"\"\n    \n    if not os.path.exists(pdf_directory):\n        os.makedirs(pdf_directory)\n    \n    if df_row['openAccessPdf']:\n        try:\n            req = requests.get(df_row['openAccessPdf']['url'])\n            if req.status_code == 200:\n                with open(pdf_directory + '/' + df_row['paperId'] + '.pdf', 'wb') as f:\n                    f.write(req.content)\n        except Exception as e:\n            pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_plain_text_from_pdf(pdf_directory='/kaggle/working/pdf_directory', txt_directory='/kaggle/working/txt_directory'):\n    all_files = glob.glob(pdf_directory + '/*.pdf')\n    \n    if not os.path.exists(txt_directory):\n        os.makedirs(txt_directory)\n        \n    for i in all_files:\n        file_id = i.split('.')[0].split('/')[-1]\n        print(i)\n        whole_text = extract_text(i)\n        with open(txt_directory + '/' + file_id + '.txt', 'w') as f:\n            f.write(whole_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[~df['openAccessPdf'].isna()].head(5).apply(extract_pdf_from_response, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_plain_text_from_pdf()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model, document_topics = perform_topic_modeling()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T00:08:53.450702Z","iopub.execute_input":"2024-03-01T00:08:53.451145Z","iopub.status.idle":"2024-03-01T00:08:58.291191Z","shell.execute_reply.started":"2024-03-01T00:08:53.451111Z","shell.execute_reply":"2024-03-01T00:08:58.289995Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Topic 0: 0.010*\"channel\" + 0.009*\"10\" + 0.009*\"OFDM\" + 0.008*\"org\" + 0.008*\"modulation\" + 0.008*\"2023\" + 0.008*\"frequency\" + 0.007*\"signal\" + 0.007*\"doi\" + 0.007*\"http\"\n\nTopic 1: 0.016*\"image\" + 0.015*\"EC50\" + 0.014*\"occupancy\" + 0.012*\"1\" + 0.012*\"0\" + 0.011*\"I\" + 0.011*\"cluster\" + 0.011*\"voxel\" + 0.010*\"level\" + 0.010*\"use\"\n\nTopic 2: 0.000*\"1\" + 0.000*\"system\" + 0.000*\"feature\" + 0.000*\"use\" + 0.000*\"5\" + 0.000*\"2\" + 0.000*\"0\" + 0.000*\"4\" + 0.000*\"I\" + 0.000*\"10\"\n\nTopic 3: 0.000*\"feature\" + 0.000*\"1\" + 0.000*\"system\" + 0.000*\"0\" + 0.000*\"use\" + 0.000*\"test\" + 0.000*\"2\" + 0.000*\"item\" + 0.000*\"datum\" + 0.000*\"3\"\n\nTopic 4: 0.023*\"feature\" + 0.013*\"system\" + 0.012*\"test\" + 0.011*\"use\" + 0.011*\"component\" + 0.011*\"item\" + 0.010*\"1\" + 0.010*\"code\" + 0.009*\"case\" + 0.009*\"function\"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"document_topics","metadata":{"execution":{"iopub.status.busy":"2024-03-01T00:08:58.293546Z","iopub.execute_input":"2024-03-01T00:08:58.294371Z","iopub.status.idle":"2024-03-01T00:08:58.302429Z","shell.execute_reply.started":"2024-03-01T00:08:58.294324Z","shell.execute_reply":"2024-03-01T00:08:58.301099Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'272cc6496c3b3de74090de7451ffcd0d19351b92.txt': [(1, 0.9995926)],\n 'fb64d10afa0c5270415abbf2f5ac33618e45e0ad.txt': [(4, 0.99987113)],\n '45272efd2973b6d1f830e6c01daeb9bbbb3769d6.txt': [(1, 0.9998108)],\n 'ecdf7d78813ef689fc0238cc9e53e0b1ef686e2f.txt': [(0, 0.99975824)]}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}