{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_pickle, DataFrame, concat\n",
    "from tqdm import tqdm\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from requests import post as post_request\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "API_KEY = getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_FIELDS = [\n",
    "    'paperId',\n",
    "    'corpusId', # Su tipado es inconsistente, podría traer errores.\n",
    "    'url',\n",
    "    'title',\n",
    "    'venue',\n",
    "    'publicationVenue',\n",
    "    'year',\n",
    "    'authors',\n",
    "    'externalIds',\n",
    "    'abstract',\n",
    "    'referenceCount',\n",
    "    'citationCount',\n",
    "    'influentialCitationCount',\n",
    "    'isOpenAccess',\n",
    "    'openAccessPdf',\n",
    "    'fieldsOfStudy',\n",
    "    's2FieldsOfStudy',\n",
    "    'publicationTypes',\n",
    "    'publicationDate',\n",
    "    'journal',\n",
    "]\n",
    "\n",
    "AUTHORS_FIELDS = [\n",
    "    'authorId',\n",
    "    'externalIds', # No siempre esta presente, podría traer errores.\n",
    "    'url',\n",
    "    'name',\n",
    "    'aliases',\n",
    "    'affiliations',\n",
    "    'paperCount',\n",
    "    'citationCount'\n",
    "]\n",
    "\n",
    "fields = GENERAL_FIELDS + ['tldr'] + [f'authors.{i}' for i in AUTHORS_FIELDS]\n",
    "fields += [f'{j}.{i}' for j in ['references', 'citations']\n",
    "           for i in GENERAL_FIELDS]\n",
    "\n",
    "QUERY_FIELDS = ','.join(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(input_list, chunk_size):\n",
    "    \"\"\"Divide una lista en sub-listas de tamaño chunk_size.\n",
    "\n",
    "    Args:\n",
    "        input_list (list): Lista original que se va a dividir.\n",
    "        chunk_size (int): Tamaño deseado para las sub-listas.\n",
    "\n",
    "    Returns:\n",
    "        list of lists: Lista de sub-listas, donde cada sub-lista tiene un tamaño de hasta chunk_size.\n",
    "    \"\"\"\n",
    "    # Usar una comprensión de lista para generar las sub-listas\n",
    "    return [input_list[i:i + chunk_size] for i in range(0, len(input_list), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_pickle('../data/api_request_results/retrieved_data.zip').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['simi_score'] = [fuzz.partial_ratio(row.title.strip().lower(), row.db_title.strip().lower()) for row in df.itertuples()]\n",
    "df.sort_values(['db_id','simi_score'], inplace=True)\n",
    "df.drop_duplicates(['db_id'], keep='last', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realmente queremos quedarnos unicamente con los papers del dataset?\n",
    "df_filter = df.query('simi_score>85') # hay falsos positivos, pero no muchos\n",
    "\n",
    "# O unicamente con los papers que son de acceso abierto (pdf descargable)\n",
    "df_filter = df_filter[df_filter.isOpenAccess].reset_index(drop=True)\n",
    "\n",
    "del df\n",
    "# usar todo => más datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1396it [00:02, 601.19it/s]\n",
      "/tmp/ipykernel_35720/2382948160.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cons_df['isOpenAccess'] = cons_df.isOpenAccess.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for row in tqdm(df_filter.itertuples()):\n",
    "    \n",
    "    refs_df = DataFrame(row.references)\n",
    "    refs_df['db_id'] = row.db_id\n",
    "    refs_df['con_type'] = 'reference'\n",
    "    \n",
    "    cits_df = DataFrame(row.citations)\n",
    "    cits_df['db_id'] = row.db_id\n",
    "    cits_df['con_type'] = 'citation'\n",
    "    \n",
    "    con_df = concat([refs_df, cits_df], ignore_index=True)\n",
    "    \n",
    "    dfs.append(con_df)\n",
    "    \n",
    "cons_df = concat(dfs, ignore_index=True)    \n",
    "cons_df.drop_duplicates('paperId', inplace=True)\n",
    "cons_df['isOpenAccess'] = cons_df.isOpenAccess.fillna(False)\n",
    "\n",
    "cons_df = cons_df[cons_df.isOpenAccess]\n",
    "cons_df = cons_df.sample(len(cons_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len:  62054\n",
      "Actual len:  62024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/6203 [01:45<27:10:51, 15.79s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Original len: \", len(cons_df))\n",
    "\n",
    "already_cons = read_pickle('../data/api_request_results/retrieved_data_connections.zip')\n",
    "\n",
    "df = cons_df[~cons_df['paperId'].isin(already_cons['paperId'])]\n",
    "df = df.sample(len(df), ignore_index=True)\n",
    "print(\"Actual len: \", len(df))\n",
    "\n",
    "del already_cons\n",
    "\n",
    "sub_lists = chunk_list(df['paperId'].to_list(), chunk_size=10)\n",
    "\n",
    "for chunk in tqdm(sub_lists):\n",
    "\n",
    "    res = post_request(\n",
    "        'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "        params={'fields': QUERY_FIELDS},\n",
    "        json={\"ids\": chunk}\n",
    "        )\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "\n",
    "        df_res = DataFrame.from_dict(data)\n",
    "\n",
    "        df_res.to_pickle(f'../data/api_request_results/connections/chunk_{df_res.paperId.iloc[0]}.zip',\n",
    "                        compression={\n",
    "                            'method': 'zip',\n",
    "                            'compresslevel': 9  # Nivel máximo de compresión para ZIP\n",
    "                        }\n",
    "                        )\n",
    "            \n",
    "        sleep(2)\n",
    "    elif res['status_code'] == 429:\n",
    "        print(\n",
    "            'Too many requests. Waiting 180 seconds')\n",
    "        sleep(180)\n",
    "    else:\n",
    "        print('Error:', res)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 37.22it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved_data = concat(\n",
    "                    [read_pickle(f) for f in tqdm(glob('../data/api_request_results/connections/*'))]\n",
    "                    )\n",
    "\n",
    "\n",
    "retrieved_data.to_pickle('../data/api_request_results/retrieved_data_connections.zip', \n",
    "                            compression= {\n",
    "                            'method': 'zip',\n",
    "                            'compresslevel': 9  # Nivel máximo de compresión para ZIP\n",
    "                            }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
