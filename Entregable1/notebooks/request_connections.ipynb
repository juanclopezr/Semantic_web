{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.utils.retrieve_data import QUERY_FIELDS\n",
    "\n",
    "from pandas import read_pickle, DataFrame, concat\n",
    "from tqdm import tqdm\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from requests import post as post_request\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "API_KEY = getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(input_list, chunk_size):\n",
    "    \"\"\"Divide una lista en sub-listas de tamaño chunk_size.\n",
    "\n",
    "    Args:\n",
    "        input_list (list): Lista original que se va a dividir.\n",
    "        chunk_size (int): Tamaño deseado para las sub-listas.\n",
    "\n",
    "    Returns:\n",
    "        list of lists: Lista de sub-listas, donde cada sub-lista tiene un tamaño de hasta chunk_size.\n",
    "    \"\"\"\n",
    "    # Usar una comprensión de lista para generar las sub-listas\n",
    "    return [input_list[i:i + chunk_size] for i in range(0, len(input_list), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_pickle('../data/api_request_results/retrieved_data.zip').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['simi_score'] = [fuzz.partial_ratio(row.title.strip().lower(), row.db_title.strip().lower()) for row in df.itertuples()]\n",
    "df.sort_values(['db_id','simi_score'], inplace=True)\n",
    "df.drop_duplicates(['db_id'], keep='last', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realmente queremos quedarnos unicamente con los papers del dataset?\n",
    "df_filter = df.query('simi_score>85') # hay falsos positivos, pero no muchos\n",
    "\n",
    "# O unicamente con los papers que son de acceso abierto (pdf descargable)\n",
    "df_filter = df_filter[df_filter.isOpenAccess].reset_index(drop=True)\n",
    "\n",
    "del df\n",
    "# usar todo => más datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1405it [00:02, 646.35it/s]\n",
      "/tmp/ipykernel_23233/2382948160.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cons_df['isOpenAccess'] = cons_df.isOpenAccess.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for row in tqdm(df_filter.itertuples()):\n",
    "    \n",
    "    refs_df = DataFrame(row.references)\n",
    "    refs_df['db_id'] = row.db_id\n",
    "    refs_df['con_type'] = 'reference'\n",
    "    \n",
    "    cits_df = DataFrame(row.citations)\n",
    "    cits_df['db_id'] = row.db_id\n",
    "    cits_df['con_type'] = 'citation'\n",
    "    \n",
    "    con_df = concat([refs_df, cits_df], ignore_index=True)\n",
    "    \n",
    "    dfs.append(con_df)\n",
    "    \n",
    "cons_df = concat(dfs, ignore_index=True)    \n",
    "cons_df.drop_duplicates('paperId', inplace=True)\n",
    "cons_df['isOpenAccess'] = cons_df.isOpenAccess.fillna(False)\n",
    "\n",
    "cons_df = cons_df[cons_df.isOpenAccess]\n",
    "cons_df = cons_df.sample(len(cons_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_df.to_pickle('../data/api_request_results/connections_catalogue.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original len: \", len(cons_df))\n",
    "\n",
    "already_cons = read_pickle('../data/api_request_results/retrieved_data_connections.zip')\n",
    "\n",
    "df = cons_df[~cons_df['paperId'].isin(already_cons['paperId'])]\n",
    "df = df.sample(len(df), ignore_index=True)\n",
    "print(\"Actual len: \", len(df))\n",
    "\n",
    "del already_cons\n",
    "\n",
    "sub_lists = chunk_list(df['paperId'].to_list(), chunk_size=3)\n",
    "\n",
    "for chunk in tqdm(sub_lists):\n",
    "\n",
    "    res = post_request(\n",
    "        'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "        params={'fields': QUERY_FIELDS},\n",
    "        json={\"ids\": chunk},\n",
    "        headers={'x-api-key': API_KEY}\n",
    "        )\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "\n",
    "        df_res = DataFrame.from_dict(data)\n",
    "\n",
    "        df_res.to_pickle(f'../data/api_request_results/connections/chunk_{df_res.paperId.iloc[0]}.zip',\n",
    "                        compression={\n",
    "                            'method': 'zip',\n",
    "                            'compresslevel': 9  # Nivel máximo de compresión para ZIP\n",
    "                        }\n",
    "                        )\n",
    "            \n",
    "        sleep(2)\n",
    "    elif res.status_code == 429:\n",
    "        print('Too many requests. Waiting 180 seconds')\n",
    "        sleep(180)\n",
    "    else:\n",
    "        try:\n",
    "            msn = res.json()['error']\n",
    "            \n",
    "            if \"maximum size\" in msn:\n",
    "                print('Maximum size reached. Waiting 10 seconds')\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                print('Error:', msn)\n",
    "                break\n",
    "        except:\n",
    "            print('Error:', res.status_code)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [00:03<00:00, 87.63it/s] \n"
     ]
    }
   ],
   "source": [
    "retrieved_data = concat(\n",
    "                [read_pickle(f) for f in tqdm(glob('../data/api_request_results/connections/*'))]\n",
    "                )\n",
    "\n",
    "\n",
    "retrieved_data.to_pickle('../data/api_request_results/retrieved_data_connections.zip', \n",
    "                            compression= {\n",
    "                            'method': 'zip',\n",
    "                            'compresslevel': 9  # Nivel máximo de compresión para ZIP\n",
    "                            }\n",
    "                    )\n",
    "\n",
    "del retrieved_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
